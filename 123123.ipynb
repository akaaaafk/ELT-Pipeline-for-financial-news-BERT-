{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-25T03:57:13.223200Z",
     "start_time": "2025-11-25T03:57:13.215200Z"
    }
   },
   "source": [
    "BASE_DIR = r\"D:\\Columbia\\Fall2025\\5400\\project\"\n",
    "\n",
    "RAW_PARQUET      = fr\"{BASE_DIR}\\news_raw.parquet\"\n",
    "CLEAN_PARQUET    = fr\"{BASE_DIR}\\news_clean.parquet\"\n",
    "NLP_INPUT_PARQUET  = fr\"{BASE_DIR}\\nlp_input.parquet\"\n",
    "NLP_OUTPUT_PARQUET = fr\"{BASE_DIR}\\nlp_enriched.parquet\"\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T04:02:45.265349Z",
     "start_time": "2025-11-25T04:00:52.987867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"5400-news-elt\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"10g\")     # 或 8g，看你机器内存\n",
    "    .config(\"spark.executor.memory\", \"10g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    .config(\"spark.default.parallelism\", \"200\")\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "BASE_DIR = r\"D:\\Columbia\\Fall2025\\5400\\project\"\n",
    "RAW_PARQUET   = fr\"{BASE_DIR}\\news_raw.parquet\"\n",
    "\n",
    "# 1. 读原始 CSV\n",
    "csv_path = r\"D:\\Columbia\\Fall2025\\5400\\project\\All_external.csv\"  # 按你实际路径改\n",
    "news_df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "\n",
    "# print(\"原始行数:\", news_df.count())\n",
    "\n",
    "# 2. 加唯一 ID（以后 NLP join 要用）\n",
    "news_df = spark.read.option(\"header\", True).csv(csv_path)\n",
    "\n",
    "# 先重分区\n",
    "news_df = news_df.repartition(200)\n",
    "\n",
    "news_df = news_df.withColumn(\"news_id\", monotonically_increasing_id())\n",
    "\n",
    "news_df.write.mode(\"overwrite\").parquet(RAW_PARQUET)"
   ],
   "id": "ae7a0a1c4a13b706",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T04:05:20.995441Z",
     "start_time": "2025-11-25T04:05:07.473227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "BASE_DIR = r\"D:\\Columbia\\Fall2025\\5400\\project\"\n",
    "RAW_PARQUET        = fr\"{BASE_DIR}\\news_raw.parquet\"\n",
    "CLEAN_PARQUET      = fr\"{BASE_DIR}\\news_clean.parquet\"\n",
    "NLP_INPUT_PARQUET  = fr\"{BASE_DIR}\\nlp_input.parquet\"\n",
    "\n",
    "news_df = spark.read.parquet(RAW_PARQUET)\n",
    "\n",
    "# 1. 保留 Article 非空的\n",
    "clean_df = news_df.filter(col(\"Article\").isNotNull())\n",
    "print(\"去除 Article 空值后行数:\", clean_df.count())\n",
    "\n",
    "# 2. 按需要下采样（比如先 100000 行）\n",
    "nlp_input_df = clean_df.limit(100000)   # 你可以改成更多，比如 500000\n",
    "\n",
    "# 3. 保存 clean layer（全量）\n",
    "clean_df.write.mode(\"overwrite\").parquet(CLEAN_PARQUET)\n",
    "\n",
    "# 4. 保存 NLP 输入子集（带 news_id + 所有你关心的列）\n",
    "nlp_input_df.write.mode(\"overwrite\").parquet(NLP_INPUT_PARQUET)\n",
    "\n",
    "print(\"clean parquet:\", CLEAN_PARQUET)\n",
    "print(\"nlp_input parquet:\", NLP_INPUT_PARQUET)\n"
   ],
   "id": "e6559d7711ccf1e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除 Article 空值后行数: 839348\n",
      "clean parquet: D:\\Columbia\\Fall2025\\5400\\project\\news_clean.parquet\n",
      "nlp_input parquet: D:\\Columbia\\Fall2025\\5400\\project\\nlp_input.parquet\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T04:08:43.804429Z",
     "start_time": "2025-11-25T04:08:39.045893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))"
   ],
   "id": "88146044db2d73df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 5070 Ti Laptop GPU\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T04:09:49.706790Z",
     "start_time": "2025-11-25T04:09:49.700791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "BASE_DIR = r\"D:\\Columbia\\Fall2025\\5400\\project\"\n",
    "NLP_INPUT_PARQUET  = fr\"{BASE_DIR}\\nlp_input.parquet\"\n",
    "NLP_OUTPUT_PARQUET = fr\"{BASE_DIR}\\nlp_enriched.parquet\"\n",
    "NLP_OUTPUT_CSV     = fr\"{BASE_DIR}\\nlp_enriched.csv\"\n",
    "\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device count:\", torch.cuda.device_count())\n",
    "device = 0 if torch.cuda.is_available() and torch.cuda.device_count() > 0 else -1\n",
    "print(\"using device =\", device)\n"
   ],
   "id": "6b424c0279999570",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.1+cu128\n",
      "cuda available: True\n",
      "device count: 1\n",
      "using device = 0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T04:09:53.086023Z",
     "start_time": "2025-11-25T04:09:52.317106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_parquet(NLP_INPUT_PARQUET)\n",
    "print(df.shape)\n",
    "print(df.columns)\n"
   ],
   "id": "b985918686a98534",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 12)\n",
      "Index(['Date', 'Article_title', 'Stock_symbol', 'Url', 'Publisher', 'Author',\n",
      "       'Article', 'Lsa_summary', 'Luhn_summary', 'Textrank_summary',\n",
      "       'Lexrank_summary', 'news_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T04:10:01.168115Z",
     "start_time": "2025-11-25T04:09:57.501212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 情感分析：英文 binary sentiment\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# 摘要模型：DistilBART\n",
    "summ_pipe = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"sshleifer/distilbart-cnn-12-6\",\n",
    "    device=device\n",
    ")\n"
   ],
   "id": "238e41f9aa5eb074",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T04:10:14.498172Z",
     "start_time": "2025-11-25T04:10:14.494532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def batch_summarize(texts, max_length=60, min_length=10):\n",
    "    \"\"\"输入 list[str]，返回 list[str] 的摘要结果\"\"\"\n",
    "    # 处理空值\n",
    "    cleaned = [t if isinstance(t, str) and t.strip() != \"\" else \"\" for t in texts]\n",
    "    # Huggingface pipeline 支持 list 输入\n",
    "    outputs = summ_pipe(\n",
    "        cleaned,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        truncation=True\n",
    "    )\n",
    "    return [o[\"summary_text\"] for o in outputs]\n",
    "\n",
    "def batch_sentiment(texts):\n",
    "    \"\"\"输入 list[str]，返回 (label, score) 两个 list\"\"\"\n",
    "    cleaned = [t if isinstance(t, str) and t.strip() != \"\" else \"\" for t in texts]\n",
    "    outputs = sentiment_pipe(cleaned)\n",
    "    labels = [o[\"label\"] for o in outputs]\n",
    "    scores = [o[\"score\"] for o in outputs]\n",
    "    return labels, scores\n"
   ],
   "id": "859f19ea770ec9d5",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T04:10:24.544600Z",
     "start_time": "2025-11-25T04:10:15.805432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "articles = df[\"Article\"].fillna(\"\").astype(str).tolist()\n",
    "n = len(articles)\n",
    "\n",
    "summary_list = []\n",
    "sent_label_list = []\n",
    "sent_score_list = []\n",
    "\n",
    "for i in tqdm(range(0, n, BATCH_SIZE), desc=\"Running NLP\"):\n",
    "    batch = articles[i:i+BATCH_SIZE]\n",
    "\n",
    "    # 摘要\n",
    "    batch_summ = batch_summarize(batch, max_length=80, min_length=15)\n",
    "    # 情感\n",
    "    batch_labels, batch_scores = batch_sentiment(batch)\n",
    "\n",
    "    summary_list.extend(batch_summ)\n",
    "    sent_label_list.extend(batch_labels)\n",
    "    sent_score_list.extend(batch_scores)\n",
    "\n",
    "# 加回 df\n",
    "df[\"article_summary\"] = summary_list\n",
    "df[\"sentiment_label\"] = sent_label_list\n",
    "df[\"sentiment_score\"] = sent_score_list\n"
   ],
   "id": "33d201496255c8a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running NLP:   0%|          | 0/6250 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (712 > 512). Running this sequence through the model will result in indexing errors\n",
      "Running NLP:   0%|          | 0/6250 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (712) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m     14\u001B[39m batch_summ = batch_summarize(batch, max_length=\u001B[32m80\u001B[39m, min_length=\u001B[32m15\u001B[39m)\n\u001B[32m     15\u001B[39m \u001B[38;5;66;03m# 情感\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m batch_labels, batch_scores = \u001B[43mbatch_sentiment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     18\u001B[39m summary_list.extend(batch_summ)\n\u001B[32m     19\u001B[39m sent_label_list.extend(batch_labels)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 17\u001B[39m, in \u001B[36mbatch_sentiment\u001B[39m\u001B[34m(texts)\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"输入 list[str]，返回 (label, score) 两个 list\"\"\"\u001B[39;00m\n\u001B[32m     16\u001B[39m cleaned = [t \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m t.strip() != \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m texts]\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m outputs = \u001B[43msentiment_pipe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcleaned\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     18\u001B[39m labels = [o[\u001B[33m\"\u001B[39m\u001B[33mlabel\u001B[39m\u001B[33m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m o \u001B[38;5;129;01min\u001B[39;00m outputs]\n\u001B[32m     19\u001B[39m scores = [o[\u001B[33m\"\u001B[39m\u001B[33mscore\u001B[39m\u001B[33m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m o \u001B[38;5;129;01min\u001B[39;00m outputs]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\envall\\all.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:168\u001B[39m, in \u001B[36mTextClassificationPipeline.__call__\u001B[39m\u001B[34m(self, inputs, **kwargs)\u001B[39m\n\u001B[32m    133\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    134\u001B[39m \u001B[33;03mClassify the text(s) given as inputs.\u001B[39;00m\n\u001B[32m    135\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    165\u001B[39m \u001B[33;03m    If `top_k` is used, one such dictionary is returned per label.\u001B[39;00m\n\u001B[32m    166\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    167\u001B[39m inputs = (inputs,)\n\u001B[32m--> \u001B[39m\u001B[32m168\u001B[39m result = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    169\u001B[39m \u001B[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001B[39;00m\n\u001B[32m    170\u001B[39m _legacy = \u001B[33m\"\u001B[39m\u001B[33mtop_k\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m kwargs\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\envall\\all.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1448\u001B[39m, in \u001B[36mPipeline.__call__\u001B[39m\u001B[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[39m\n\u001B[32m   1444\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m can_use_iterator:\n\u001B[32m   1445\u001B[39m     final_iterator = \u001B[38;5;28mself\u001B[39m.get_iterator(\n\u001B[32m   1446\u001B[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001B[32m   1447\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1448\u001B[39m     outputs = \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfinal_iterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1449\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n\u001B[32m   1450\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\envall\\all.venv\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:126\u001B[39m, in \u001B[36mPipelineIterator.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    123\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.loader_batch_item()\n\u001B[32m    125\u001B[39m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m126\u001B[39m item = \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m.iterator)\n\u001B[32m    127\u001B[39m processed = \u001B[38;5;28mself\u001B[39m.infer(item, **\u001B[38;5;28mself\u001B[39m.params)\n\u001B[32m    128\u001B[39m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\envall\\all.venv\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:127\u001B[39m, in \u001B[36mPipelineIterator.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    125\u001B[39m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[32m    126\u001B[39m item = \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m.iterator)\n\u001B[32m--> \u001B[39m\u001B[32m127\u001B[39m processed = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minfer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    128\u001B[39m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n\u001B[32m    129\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.loader_batch_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    130\u001B[39m     \u001B[38;5;66;03m# Try to infer the size of the batch\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\envall\\all.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1374\u001B[39m, in \u001B[36mPipeline.forward\u001B[39m\u001B[34m(self, model_inputs, **forward_params)\u001B[39m\n\u001B[32m   1372\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[32m   1373\u001B[39m         model_inputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_inputs, device=\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m-> \u001B[39m\u001B[32m1374\u001B[39m         model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1375\u001B[39m         model_outputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m   1376\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\envall\\all.venv\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:199\u001B[39m, in \u001B[36mTextClassificationPipeline._forward\u001B[39m\u001B[34m(self, model_inputs)\u001B[39m\n\u001B[32m    197\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33muse_cache\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m inspect.signature(model_forward).parameters:\n\u001B[32m    198\u001B[39m     model_inputs[\u001B[33m\"\u001B[39m\u001B[33muse_cache\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m199\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\envall\\all.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\envall\\all.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\envall\\all.venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:905\u001B[39m, in \u001B[36mDistilBertForSequenceClassification.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m    897\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    898\u001B[39m \u001B[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[32m    899\u001B[39m \u001B[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[32m    900\u001B[39m \u001B[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[32m    901\u001B[39m \u001B[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[32m    902\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    903\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m--> \u001B[39m\u001B[32m905\u001B[39m distilbert_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdistilbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    906\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    907\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    908\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    909\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    910\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    911\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    912\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    913\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    914\u001B[39m hidden_state = distilbert_output[\u001B[32m0\u001B[39m]  \u001B[38;5;66;03m# (bs, seq_len, dim)\u001B[39;00m\n\u001B[32m    915\u001B[39m pooled_output = hidden_state[:, \u001B[32m0\u001B[39m]  \u001B[38;5;66;03m# (bs, dim)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\envall\\all.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\envall\\all.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\envall\\all.venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:711\u001B[39m, in \u001B[36mDistilBertModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m    708\u001B[39m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[32m    709\u001B[39m head_mask = \u001B[38;5;28mself\u001B[39m.get_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m.config.num_hidden_layers)\n\u001B[32m--> \u001B[39m\u001B[32m711\u001B[39m embeddings = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# (bs, seq_length, dim)\u001B[39;00m\n\u001B[32m    713\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config._attn_implementation == \u001B[33m\"\u001B[39m\u001B[33mflash_attention_2\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    714\u001B[39m     attention_mask = attention_mask \u001B[38;5;28;01mif\u001B[39;00m (attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[32m0\u001B[39m \u001B[38;5;129;01min\u001B[39;00m attention_mask) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\envall\\all.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\envall\\all.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\envall\\all.venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:125\u001B[39m, in \u001B[36mEmbeddings.forward\u001B[39m\u001B[34m(self, input_ids, input_embeds)\u001B[39m\n\u001B[32m    121\u001B[39m     position_ids = position_ids.unsqueeze(\u001B[32m0\u001B[39m).expand_as(input_ids)  \u001B[38;5;66;03m# (bs, max_seq_length)\u001B[39;00m\n\u001B[32m    123\u001B[39m position_embeddings = \u001B[38;5;28mself\u001B[39m.position_embeddings(position_ids)  \u001B[38;5;66;03m# (bs, max_seq_length, dim)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m embeddings = \u001B[43minput_embeds\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_embeddings\u001B[49m  \u001B[38;5;66;03m# (bs, max_seq_length, dim)\u001B[39;00m\n\u001B[32m    126\u001B[39m embeddings = \u001B[38;5;28mself\u001B[39m.LayerNorm(embeddings)  \u001B[38;5;66;03m# (bs, max_seq_length, dim)\u001B[39;00m\n\u001B[32m    127\u001B[39m embeddings = \u001B[38;5;28mself\u001B[39m.dropout(embeddings)  \u001B[38;5;66;03m# (bs, max_seq_length, dim)\u001B[39;00m\n",
      "\u001B[31mRuntimeError\u001B[39m: The size of tensor a (712) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
