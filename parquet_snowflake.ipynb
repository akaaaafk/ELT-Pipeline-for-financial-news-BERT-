{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-28T03:53:05.270374Z",
     "start_time": "2025-11-28T03:53:05.106434Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "import os\n",
    "\n",
    "def create_spark(app_name=\"5400-news-elt\"):\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(app_name)\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.driver.memory\", \"12g\")\n",
    "        .config(\"spark.executor.memory\", \"12g\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"400\")\n",
    "        .config(\"spark.default.parallelism\", \"400\")\n",
    "        .config(\"spark.sql.files.maxRecordsPerFile\", \"200000\")\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "        .config(\"spark.sql.parquet.binaryAsString\", \"false\")\n",
    "        .config(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "        .config(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "\n",
    "\n",
    "        # Snowflake JAR\n",
    "        .config(\n",
    "            \"spark.jars.packages\",\n",
    "            \"net.snowflake:snowflake-jdbc:3.15.1,\"\n",
    "            \"net.snowflake:spark-snowflake_2.12:2.11.0-sf-1.5.0\"\n",
    "        )\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    print(\"ðŸ”¥ Spark Ready:\", spark.version)\n",
    "    return spark\n",
    "\n",
    "# Snowflake config\n",
    "sfOptions = {\n",
    "    \"sfURL\": \"pobriux-fyc84817.snowflakecomputing.com\",\n",
    "    \"sfUser\": \"AKAAAAFK\",\n",
    "    \"sfAuthenticator\": \"externalbrowser\",\n",
    "    \"sfRole\": \"ACCOUNTADMIN\",\n",
    "    \"sfWarehouse\": \"SNOWFLAKE_LEARNING_WH\",\n",
    "    \"sfDatabase\": \"SNOWFLAKE_LEARNING_DB\",\n",
    "    \"sfSchema\": \"PUBLIC\",\n",
    "}\n",
    "\n",
    "def write_sf(df, table_name, mode=\"overwrite\"):\n",
    "    (\n",
    "        df.write\n",
    "        .format(\"snowflake\")\n",
    "        .options(**sfOptions)\n",
    "        .option(\"dbtable\", table_name)\n",
    "        .mode(mode)\n",
    "        .save()\n",
    "    )\n",
    "    print(f\"â„ å†™å…¥ Snowflake â†’ {table_name} ï¼ˆmode={mode})\")\n",
    "\n",
    "# è·¯å¾„\n",
    "BASE = r\"D:\\Columbia\\Fall2025\\5400\\project\\layer\"\n",
    "CSV_PATH = r\"D:\\Columbia\\Fall2025\\5400\\project\\All_external.csv\"\n",
    "\n",
    "BRONZE = os.path.join(BASE, \"bronze\")\n",
    "SILVER = os.path.join(BASE, \"silver\")\n",
    "GOLD = os.path.join(BASE, \"gold\")\n",
    "NLP_SILVER = os.path.join(BASE, \"silver_for_nlp\")\n",
    "SENTIMENT_SAMPLE = os.path.join(BASE, \"sentiment_sample\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:53:05.285685Z",
     "start_time": "2025-11-28T03:53:05.275881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_bronze(spark):\n",
    "    print(\"ðŸ“¥ Loading CSV...\")\n",
    "    df = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .options(\n",
    "            header=\"true\",\n",
    "            inferSchema=\"true\",\n",
    "            multiLine=\"true\",\n",
    "            escape=\"\\\"\",\n",
    "            quote=\"\\\"\",\n",
    "            mode=\"PERMISSIVE\"\n",
    "        )\n",
    "        .load(CSV_PATH)\n",
    "    )\n",
    "\n",
    "    for c, t in df.dtypes:\n",
    "        if t == \"string\":\n",
    "            df = df.withColumn(c, F.trim(F.col(c)))\n",
    "\n",
    "    if \"Date\" in df.columns:\n",
    "        df = df.withColumn(\"Date\", F.to_date(\"Date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "    df = df.repartition(400)\n",
    "\n",
    "    print(\"ðŸ’¾ Writing Bronze â†’ Snowflake\")\n",
    "    write_sf(df, \"NEWS_BRONZE\")\n",
    "\n",
    "    return df"
   ],
   "id": "e9a3bd1834e98e3e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:53:05.300923Z",
     "start_time": "2025-11-28T03:53:05.290648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_silver(spark):\n",
    "    df = spark.read.parquet(BRONZE)\n",
    "\n",
    "    if \"Date\" in df.columns:\n",
    "        df = df.filter(F.col(\"Date\").isNotNull())\n",
    "    if \"Stock_symbol\" in df.columns:\n",
    "        df = df.filter(F.col(\"Stock_symbol\").isNotNull())\n",
    "\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    if \"Publisher\" in df.columns:\n",
    "        df = df.withColumn(\"Publisher_norm\", F.upper(\"Publisher\"))\n",
    "    if \"Author\" in df.columns:\n",
    "        df = df.withColumn(\"Author_norm\", F.upper(\"Author\"))\n",
    "\n",
    "    df = df.withColumn(\"news_id\", F.monotonically_increasing_id())\n",
    "    df = df.repartition(400, \"Stock_symbol\", \"Date\")\n",
    "\n",
    "    print(\"ðŸ’¾ Writing Silver â†’ Snowflake\")\n",
    "    write_sf(df, \"NEWS_SILVER\")\n",
    "\n",
    "    return df"
   ],
   "id": "df0d130c332d1bc3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:53:05.316120Z",
     "start_time": "2025-11-28T03:53:05.305906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_gold(spark):\n",
    "    df = spark.read.parquet(SILVER)\n",
    "\n",
    "    df = df.withColumn(\"title_len\", F.length(\"Article_title\"))\n",
    "\n",
    "    agg_cols = [\n",
    "        F.count(\"*\").alias(\"article_count\"),\n",
    "        F.countDistinct(\"Publisher\").alias(\"publisher_count\"),\n",
    "        F.avg(\"title_len\").alias(\"avg_title_len\"),\n",
    "    ]\n",
    "\n",
    "    for col in [\"Textrank_summary\", \"Lsa_summary\", \"Luhn_summary\", \"Lexrank_summary\"]:\n",
    "        if col in df.columns:\n",
    "            agg_cols.append(F.first(col, ignorenulls=True).alias(\"sample_summary\"))\n",
    "            break\n",
    "\n",
    "    gold_df = df.groupBy(\"Date\", \"Stock_symbol\").agg(*agg_cols)\n",
    "    gold_df = gold_df.repartition(200, \"Stock_symbol\")\n",
    "\n",
    "    print(\"ðŸ’¾ Writing Gold â†’ Snowflake\")\n",
    "    write_sf(gold_df, \"NEWS_GOLD\")\n",
    "\n",
    "    return gold_df"
   ],
   "id": "f4a88ebc4b5c84c1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:53:05.331401Z",
     "start_time": "2025-11-28T03:53:05.321327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_silver_nlp(spark):\n",
    "    df = spark.read.parquet(SILVER)\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"text\",\n",
    "        F.coalesce(\n",
    "            F.col(\"Article\"),\n",
    "            F.col(\"Textrank_summary\"),\n",
    "            F.col(\"Lsa_summary\"),\n",
    "            F.col(\"Luhn_summary\"),\n",
    "            F.col(\"Lexrank_summary\"),\n",
    "            F.col(\"Article_title\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df = df.filter(F.col(\"text\").isNotNull() & (F.length(F.trim(\"text\")) > 0))\n",
    "\n",
    "    df = df.select(\n",
    "        \"news_id\", \"Date\", \"Article_title\", \"Stock_symbol\",\n",
    "        \"Publisher\", \"Author\", \"Url\", \"text\"\n",
    "    ).repartition(200)\n",
    "\n",
    "    print(\"ðŸ§  Writing silver_for_nlp â†’ Snowflake\")\n",
    "    write_sf(df, \"NEWS_SILVER_FOR_NLP\")\n",
    "\n",
    "    return df"
   ],
   "id": "7e47e3cc9a36086e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:53:05.346701Z",
     "start_time": "2025-11-28T03:53:05.336517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def join_silver_sentiment_sample(spark):\n",
    "    silver_df = spark.read.parquet(SILVER)\n",
    "    sent_df = spark.read.parquet(SENTIMENT_SAMPLE)\n",
    "\n",
    "    silver_with_sent = silver_df.join(sent_df, on=\"news_id\", how=\"left\")\n",
    "    silver_with_sent = silver_with_sent.repartition(200, \"Stock_symbol\", \"Date\")\n",
    "\n",
    "    print(\"ðŸ“¤ å†™å…¥ Snowflakeï¼šNEWS_SILVER_WITH_SENT_SAMPLE\")\n",
    "    write_sf(silver_with_sent, \"NEWS_SILVER_WITH_SENT_SAMPLE\")\n",
    "\n",
    "    return silver_with_sent"
   ],
   "id": "9fd9de23667ea08d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:53:05.361983Z",
     "start_time": "2025-11-28T03:53:05.352782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_gold_with_sentiment(spark):\n",
    "    silver_sent = join_silver_sentiment_sample(spark)\n",
    "    gold_df = spark.read.parquet(GOLD)\n",
    "\n",
    "    sent_agg = (\n",
    "        silver_sent\n",
    "        .groupBy(\"Date\", \"Stock_symbol\")\n",
    "        .agg(\n",
    "            F.avg(\"sentiment_score_signed\").alias(\"avg_sentiment_score\"),\n",
    "            F.sum(F.when(F.col(\"sentiment_label\") == \"POSITIVE\", 1).otherwise(0)).alias(\"pos_article_count\"),\n",
    "            F.sum(F.when(F.col(\"sentiment_label\") == \"NEGATIVE\", 1).otherwise(0)).alias(\"neg_article_count\"),\n",
    "            F.count(F.col(\"sentiment_label\")).alias(\"sentiment_article_count\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    gold_with_sent = (\n",
    "        gold_df.alias(\"g\")\n",
    "        .join(sent_agg.alias(\"s\"), on=[\"Date\", \"Stock_symbol\"], how=\"left\")\n",
    "    )\n",
    "\n",
    "    gold_with_sent = gold_with_sent.withColumn(\n",
    "        \"sentiment_coverage\",\n",
    "        F.when(\n",
    "            F.col(\"article_count\") > 0,\n",
    "            F.col(\"sentiment_article_count\").cast(\"double\") / F.col(\"article_count\")\n",
    "        ).otherwise(F.lit(0.0))\n",
    "    )\n",
    "\n",
    "    gold_with_sent = gold_with_sent.repartition(200, \"Stock_symbol\")\n",
    "\n",
    "    print(\"ðŸ† å†™å…¥ Snowflakeï¼šNEWS_GOLD_WITH_SENT\")\n",
    "    write_sf(gold_with_sent, \"NEWS_GOLD_WITH_SENT\")\n",
    "\n",
    "    return gold_with_sent"
   ],
   "id": "452c2348d8872d93",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:53:08.269115Z",
     "start_time": "2025-11-28T03:53:05.592668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = create_spark()\n",
    "\n",
    "bronze_df = build_bronze(spark)"
   ],
   "id": "867adb923d1c8a63",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m spark \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_spark\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m bronze_df \u001B[38;5;241m=\u001B[39m build_bronze(spark)\n",
      "Cell \u001B[1;32mIn[1], line 6\u001B[0m, in \u001B[0;36mcreate_spark\u001B[1;34m(app_name)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcreate_spark\u001B[39m(app_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m5400-news-elt\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m      5\u001B[0m     spark \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m----> 6\u001B[0m         \u001B[43mSparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuilder\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mapp_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaster\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlocal[*]\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspark.driver.memory\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m12g\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspark.executor.memory\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m12g\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspark.sql.shuffle.partitions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m400\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspark.default.parallelism\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m400\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspark.sql.files.maxRecordsPerFile\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m200000\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspark.sql.parquet.compression.codec\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msnappy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     15\u001B[0m \n\u001B[0;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# â­ Snowflake JAR è‡ªåŠ¨ä¸‹è½½\u001B[39;49;00m\n\u001B[0;32m     17\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mspark.jars.packages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnet.snowflake:snowflake-jdbc:3.15.1,\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m     20\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnet.snowflake:spark-snowflake_2.12:2.11.0-sf-1.5.0\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m     21\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m     )\n\u001B[0;32m     24\u001B[0m     spark\u001B[38;5;241m.\u001B[39msparkContext\u001B[38;5;241m.\u001B[39msetLogLevel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWARN\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     25\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mðŸ”¥ Spark Ready:\u001B[39m\u001B[38;5;124m\"\u001B[39m, spark\u001B[38;5;241m.\u001B[39mversion)\n",
      "File \u001B[1;32mD:\\Code\\ANACONDA3\\envs\\pyspark1\\lib\\site-packages\\pyspark\\sql\\session.py:269\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    267\u001B[0m     sparkConf\u001B[38;5;241m.\u001B[39mset(key, value)\n\u001B[0;32m    268\u001B[0m \u001B[38;5;66;03m# This SparkContext may be an existing one.\u001B[39;00m\n\u001B[1;32m--> 269\u001B[0m sc \u001B[38;5;241m=\u001B[39m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetOrCreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msparkConf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[39;00m\n\u001B[0;32m    271\u001B[0m \u001B[38;5;66;03m# by all sessions.\u001B[39;00m\n\u001B[0;32m    272\u001B[0m session \u001B[38;5;241m=\u001B[39m SparkSession(sc, options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options)\n",
      "File \u001B[1;32mD:\\Code\\ANACONDA3\\envs\\pyspark1\\lib\\site-packages\\pyspark\\context.py:483\u001B[0m, in \u001B[0;36mSparkContext.getOrCreate\u001B[1;34m(cls, conf)\u001B[0m\n\u001B[0;32m    481\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    482\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 483\u001B[0m         \u001B[43mSparkContext\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mSparkConf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    484\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    485\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\n",
      "File \u001B[1;32mD:\\Code\\ANACONDA3\\envs\\pyspark1\\lib\\site-packages\\pyspark\\context.py:195\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001B[0m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    190\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    191\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    192\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    193\u001B[0m     )\n\u001B[1;32m--> 195\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    197\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(\n\u001B[0;32m    198\u001B[0m         master,\n\u001B[0;32m    199\u001B[0m         appName,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    208\u001B[0m         udf_profiler_cls,\n\u001B[0;32m    209\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Code\\ANACONDA3\\envs\\pyspark1\\lib\\site-packages\\pyspark\\context.py:417\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[1;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[0;32m    415\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    416\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_gateway:\n\u001B[1;32m--> 417\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_gateway \u001B[38;5;241m=\u001B[39m gateway \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mlaunch_gateway\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    418\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_gateway\u001B[38;5;241m.\u001B[39mjvm\n\u001B[0;32m    420\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m instance:\n",
      "File \u001B[1;32mD:\\Code\\ANACONDA3\\envs\\pyspark1\\lib\\site-packages\\pyspark\\java_gateway.py:106\u001B[0m, in \u001B[0;36mlaunch_gateway\u001B[1;34m(conf, popen_kwargs)\u001B[0m\n\u001B[0;32m    103\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.1\u001B[39m)\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(conn_info_file):\n\u001B[1;32m--> 106\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJava gateway process exited before sending its port number\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(conn_info_file, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m info:\n\u001B[0;32m    109\u001B[0m     gateway_port \u001B[38;5;241m=\u001B[39m read_int(info)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "df7985d98b51cf51"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
