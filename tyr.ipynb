{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-23T22:16:58.079930Z",
     "start_time": "2025-11-23T22:16:57.980010Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set Java 17 as the Java version for PySpark\n",
    "# PySpark 3.5.7 requires Java 17, can't do with Java 25\n",
    "os.environ['JAVA HOME'] = '/Program Files/Eclipse Adoptium/jdk-17.0.17.10-hotspot'\n",
    "os.environ['PATH'] = os.environ['JAVA_HOME'] + '/bin;' + os.environ['PATH']\n",
    "\n",
    "# Verify Java version\n",
    "import subprocess\n",
    "result = subprocess.run(['java','-version'], capture_output = True,text=True)\n",
    "print(\"Java version:\", result.stderr.split('\\n')[0])\n",
    "\n",
    "# Set Python executable for PySpark\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java version: openjdk version \"17.0.17\" 2025-10-21\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:16:59.903368Z",
     "start_time": "2025-11-23T22:16:59.893050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ],
   "id": "23feab7dc2345c1b",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:17:00.751012Z",
     "start_time": "2025-11-23T22:17:00.740927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "print(\"HADOOP_HOME =\", os.environ.get(\"HADOOP_HOME\"))\n",
    "\n",
    "if os.environ.get(\"HADOOP_HOME\"):\n",
    "    path = os.path.join(os.environ[\"HADOOP_HOME\"], \"bin\", \"winutils.exe\")\n",
    "    print(\"winutils exists:\", os.path.exists(path), \"->\", path)\n",
    "else:\n",
    "    print(\"❌ Hadoop not configured in this session\")"
   ],
   "id": "fad8faa654efdb30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HADOOP_HOME = D:\\Code\\hadoop-3.4.0\\hadoop-3.4.0\n",
      "winutils exists: True -> D:\\Code\\hadoop-3.4.0\\hadoop-3.4.0\\bin\\winutils.exe\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:17:02.515481Z",
     "start_time": "2025-11-23T22:17:02.497963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark in venv\") \\\n",
    "    .config(\"spark.cores.max\", \"4\") \\\n",
    "    .config('spark.executor.memory', '8G') \\\n",
    "    .config('spark.driver.maxResultSize', '8g') \\\n",
    "    .config('spark.kryoserializer.buffer.max','512m') \\\n",
    "    .config(\"spark.driver.cores\", \"4\") \\\n",
    "    .config(\"spark.pyspark.python\", sys.executable)\\\n",
    "    .config(\"spark.pyspark.driver.python\", sys.executable)\\\n",
    "    .config(\"spark.python.use.daemon\", \"false\") \\\n",
    "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext  # SparkContext对象\n",
    "\n",
    "print(\"Using Apache Spark Version\", spark.version)\n",
    "print(sys.executable)"
   ],
   "id": "9301802f211280e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apache Spark Version 3.5.7\n",
      "D:\\Columbia\\Fall2025\\5400\\SQL.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:17:10.084643Z",
     "start_time": "2025-11-23T22:17:03.429154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = spark.read.format(\"csv\").options(\n",
    "    header = 'true',\n",
    "    inferschema = 'true',\n",
    "    treatEmptyValuesAsNulls = 'true'\n",
    ").load('All_external.csv')\n",
    "df.printSchema()"
   ],
   "id": "6c4f44565057b2af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Article_title: string (nullable = true)\n",
      " |-- Stock_symbol: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Publisher: string (nullable = true)\n",
      " |-- Author: string (nullable = true)\n",
      " |-- Article: string (nullable = true)\n",
      " |-- Lsa_summary: string (nullable = true)\n",
      " |-- Luhn_summary: string (nullable = true)\n",
      " |-- Textrank_summary: string (nullable = true)\n",
      " |-- Lexrank_summary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:17:11.592705Z",
     "start_time": "2025-11-23T22:17:10.091301Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Count of all records:\", df.count())",
   "id": "88c2d8002b9568f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of all records: 29984720\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:17:11.623962Z",
     "start_time": "2025-11-23T22:17:11.598704Z"
    }
   },
   "cell_type": "code",
   "source": "df.columns",
   "id": "aed8808a579ee7cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date',\n",
       " 'Article_title',\n",
       " 'Stock_symbol',\n",
       " 'Url',\n",
       " 'Publisher',\n",
       " 'Author',\n",
       " 'Article',\n",
       " 'Lsa_summary',\n",
       " 'Luhn_summary',\n",
       " 'Textrank_summary',\n",
       " 'Lexrank_summary']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Bronze Layer",
   "id": "4f63aae9ec7e54e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T20:05:49.122998Z",
     "start_time": "2025-11-23T20:05:48.922342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n"
   ],
   "id": "4614730376b549b5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:17:13.415691Z",
     "start_time": "2025-11-23T22:17:13.291141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize MongoDB and database\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost',27017)\n",
    "db = client.apan5400"
   ],
   "id": "daa2ce0ef3b99f5c",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client[\"news_elt\"]       # 数据库名字\n",
    "col = db[\"fnspid_bronze\"]     # Bronze 层 collection\n"
   ],
   "id": "a5f6348348a8e88e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "col.delete_many({})  # 覆盖写，可选\n",
    "col.insert_many(df_bronze_pd.to_dict(\"records\"))\n",
    "\n",
    "print(\"✅ Bronze 层写入到 Docker MongoDB 完成\")\n"
   ],
   "id": "4e07736ead283999"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:19:29.524375Z",
     "start_time": "2025-11-23T22:19:26.296868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ⭐ 请将此路径替换为您主机上 ALL_external.csv 文件的实际路径\n",
    "csv_file = r'D:\\Columbia\\Fall2025\\5400\\project\\ALL_external.csv'\n",
    "parquet_dir = r'D:\\Columbia\\Fall2025\\5400\\project\\bronze_parquet_output'\n",
    "\n",
    "# 确保输出目录存在\n",
    "os.makedirs(parquet_dir, exist_ok=True)\n",
    "print(f\"Reading from: {csv_file}\")\n",
    "print(f\"Writing to: {parquet_dir}\")\n",
    "\n",
    "# --- 分块读取 CSV 并写入 Parquet ---\n",
    "chunksize = 1000000  # 每次读取 100 万行\n",
    "chunk_num = 0\n",
    "\n",
    "try:\n",
    "    for chunk in pd.read_csv(csv_file, chunksize=chunksize, low_memory=False, dtype=str):\n",
    "        output_file = os.path.join(parquet_dir, f'part-{chunk_num:05d}.parquet')\n",
    "        chunk.to_parquet(output_file, index=False, engine='pyarrow')\n",
    "        chunk_num += 1\n",
    "        print(f\"Processed chunk {chunk_num}. Saved to {output_file}\")\n",
    "    print(\"✅ Bronze 层 Pandas 转换完成！\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ FATAL ERROR in Pandas conversion: {e}\")"
   ],
   "id": "c9547d2d68ce9199",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from: D:\\Columbia\\Fall2025\\5400\\project\\ALL_external.csv\n",
      "Writing to: D:\\Columbia\\Fall2025\\5400\\project\\bronze_parquet_output\n",
      "❌ FATAL ERROR in Pandas conversion: No type extension with name arrow.py_extension_type found\n"
     ]
    }
   ],
   "execution_count": 47
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
