{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-28T03:48:16.741805Z",
     "start_time": "2025-11-28T03:48:16.729424Z"
    }
   },
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. è·¯å¾„å®šä¹‰ (æ ¹æ®æ‚¨çš„æœ¬åœ°ç¯å¢ƒ)\n",
    "# -------------------------------------------------------------------------\n",
    "BASE = r\"D:\\Columbia\\Fall2025\\5400\\project\\layer\"\n",
    "CSV_PATH = r\"D:\\Columbia\\Fall2025\\5400\\project\\All_external.csv\"\n",
    "\n",
    "# å„å±‚çº§è·¯å¾„\n",
    "BRONZE = os.path.join(BASE, \"bronze\")\n",
    "SILVER = os.path.join(BASE, \"silver\")\n",
    "GOLD = os.path.join(BASE, \"gold\")\n",
    "NLP_SILVER = os.path.join(BASE, \"silver_for_nlp\")\n",
    "SENTIMENT_OUTPUT = os.path.join(BASE, \"sentiment_output\")       # æ–°å¢: NLP è¾“å‡º\n",
    "FACT_NEWS_SENTIMENT = os.path.join(BASE, \"fact_news_sentiment\") # æ–°å¢: æœ€ç»ˆäº‹å®è¡¨\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Spark Session åˆå§‹åŒ–\n",
    "# -------------------------------------------------------------------------\n",
    "def create_spark(app_name=\"5400-news-elt-optimized\"):\n",
    "    # æ³¨æ„ï¼šä¸ºäº†è¿æ¥ Snowflakeï¼Œéœ€è¦åŠ è½½å¯¹åº”çš„ Maven åŒ…\n",
    "    # å¦‚æœæ‚¨æœ¬åœ°ç½‘ç»œæ— æ³•è‡ªåŠ¨ä¸‹è½½ JARï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨ä¸‹è½½å¹¶é…ç½® spark.jars\n",
    "    snowflake_package = \"net.snowflake:spark-snowflake_2.12:2.11.0-spark_3.3\"\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(app_name)\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.driver.memory\", \"12g\")\n",
    "        .config(\"spark.executor.memory\", \"12g\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"400\")\n",
    "        .config(\"spark.default.parallelism\", \"400\")\n",
    "        # æ·»åŠ  Snowflake ä¾èµ– (æ ¹æ®æ‚¨çš„ Spark ç‰ˆæœ¬è°ƒæ•´ Scala/Spark ç‰ˆæœ¬å·)\n",
    "        .config(\"spark.jars.packages\", snowflake_package)\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    return spark"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:48:27.089740Z",
     "start_time": "2025-11-28T03:48:27.074813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_bronze(spark):\n",
    "    \"\"\"Raw CSV -> Parquet (Bronze Layer)\"\"\"\n",
    "    print(\"ğŸš€ Building Bronze Layer...\")\n",
    "    if os.path.exists(BRONZE): shutil.rmtree(BRONZE)\n",
    "\n",
    "    df = spark.read.option(\"header\", \"true\").csv(CSV_PATH)\n",
    "\n",
    "    # ç®€å•çš„æ¸…æ´—ï¼šå»é™¤ç©ºæ ¼ï¼Œè½¬æ¢æ—¥æœŸ\n",
    "    for col_name in df.columns:\n",
    "        df = df.withColumn(col_name, F.trim(F.col(col_name)))\n",
    "\n",
    "    df = df.withColumn(\"Date\", F.to_date(F.col(\"Date\"), \"yyyy-mm-dd\"))\n",
    "\n",
    "    df.write.mode(\"overwrite\").parquet(BRONZE)\n",
    "    print(\"âœ… Bronze Layer Saved:\", BRONZE)\n",
    "    return df\n",
    "\n",
    "def build_silver(spark):\n",
    "    \"\"\"Bronze -> Silver (Cleaning & Deduplication)\"\"\"\n",
    "    print(\"ğŸš€ Building Silver Layer...\")\n",
    "    if os.path.exists(SILVER): shutil.rmtree(SILVER)\n",
    "\n",
    "    df = spark.read.parquet(BRONZE)\n",
    "\n",
    "    # è¿‡æ»¤æ— æ•ˆæ•°æ®\n",
    "    df = df.filter(F.col(\"Date\").isNotNull() & F.col(\"Stock_symbol\").isNotNull())\n",
    "    df = df.dropDuplicates([\"Date\", \"Stock_symbol\", \"Article_title\"])\n",
    "\n",
    "    # æ ‡å‡†åŒ–æ–‡æœ¬\n",
    "    df = df.withColumn(\"Publisher_norm\", F.upper(F.trim(F.col(\"Publisher\")))) \\\n",
    "           .withColumn(\"Author_norm\", F.upper(F.trim(F.col(\"Author\"))))\n",
    "\n",
    "    # ç”Ÿæˆå”¯ä¸€ ID (é‡è¦ï¼šåç»­ Join çš„ Key)\n",
    "    df = df.withColumn(\"news_id\", F.md5(F.concat_ws(\"|\", \"Date\", \"Stock_symbol\", \"Article_title\")))\n",
    "\n",
    "    df.write.mode(\"overwrite\").parquet(SILVER)\n",
    "    print(\"âœ… Silver Layer Saved:\", SILVER)\n",
    "    return df\n",
    "\n",
    "def build_silver_nlp(spark):\n",
    "    \"\"\"Silver -> NLP Ready Data (Prepare text for model)\"\"\"\n",
    "    print(\"ğŸš€ Preparing Data for NLP...\")\n",
    "    if os.path.exists(NLP_SILVER): shutil.rmtree(NLP_SILVER)\n",
    "\n",
    "    df = spark.read.parquet(SILVER)\n",
    "\n",
    "    # æ‹¼æ¥æ ‡é¢˜å’Œæ‘˜è¦ä½œä¸ºæ¨¡å‹è¾“å…¥\n",
    "    nlp_df = df.select(\n",
    "        \"news_id\",\n",
    "        F.concat_ws(\" . \", F.col(\"Article_title\"), F.col(\"Lsa_summary\")).alias(\"text\")\n",
    "    ).filter(\"text != ''\")\n",
    "\n",
    "    nlp_df.write.mode(\"overwrite\").parquet(NLP_SILVER)\n",
    "    print(\"âœ… NLP Ready Data Saved:\", NLP_SILVER)\n",
    "    return nlp_df"
   ],
   "id": "2eb134529101f0ff",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:48:28.269433Z",
     "start_time": "2025-11-28T03:48:28.256853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_sentiment_features_scalable(spark):\n",
    "    \"\"\"\n",
    "    ã€ä¼˜åŒ–ã€‘ä½¿ç”¨ Spark Pandas UDF æ‰§è¡Œåˆ†å¸ƒå¼æƒ…æ„Ÿåˆ†æ\n",
    "    æ›¿ä»£åŸæ¥çš„å•æœºå¾ªç¯ï¼Œæ€§èƒ½æå‡å·¨å¤§ã€‚\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ Running Scalable NLP Sentiment Analysis...\")\n",
    "    import torch\n",
    "    import pandas as pd\n",
    "    from pyspark.sql.functions import pandas_udf\n",
    "    from transformers import pipeline\n",
    "\n",
    "    # å¦‚æœå·²å­˜åœ¨åˆ™æ¸…ç†\n",
    "    if os.path.exists(SENTIMENT_OUTPUT): shutil.rmtree(SENTIMENT_OUTPUT)\n",
    "\n",
    "    # è¯»å–å‡†å¤‡å¥½çš„æ–‡æœ¬æ•°æ®\n",
    "    df = spark.read.parquet(NLP_SILVER)\n",
    "\n",
    "    # å®šä¹‰ UDF è¾“å‡ºç»“æ„\n",
    "    schema = StructType([\n",
    "        StructField(\"label\", StringType(), True),\n",
    "        StructField(\"score\", DoubleType(), True),\n",
    "        StructField(\"score_signed\", DoubleType(), True)\n",
    "    ])\n",
    "\n",
    "    # å®šä¹‰ Pandas UDF (å°†åœ¨æ¯ä¸ª Worker èŠ‚ç‚¹æ‰§è¡Œ)\n",
    "    @pandas_udf(schema)\n",
    "    def sentiment_udf(texts: pd.Series) -> pd.DataFrame:\n",
    "        # åœ¨ Worker ä¸ŠåŠ è½½æ¨¡å‹ (å»ºè®®ä½¿ç”¨è½»é‡çº§æ¨¡å‹å¦‚ distilbert)\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "        pipe = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=device)\n",
    "\n",
    "        # æ‰¹é‡æ¨ç†\n",
    "        preds = pipe(texts.tolist(), truncation=True, max_length=256, batch_size=32)\n",
    "\n",
    "        labels = [p['label'] for p in preds]\n",
    "        scores = [p['score'] for p in preds]\n",
    "\n",
    "        # è®¡ç®—å¸¦ç¬¦å·åˆ†æ•°\n",
    "        signed_scores = []\n",
    "        for l, s in zip(labels, scores):\n",
    "            signed_scores.append(s if l == 'POSITIVE' else -s)\n",
    "\n",
    "        return pd.DataFrame({'label': labels, 'score': scores, 'score_signed': signed_scores})\n",
    "\n",
    "    # åº”ç”¨ UDF\n",
    "    result_df = df.withColumn(\"sentiment\", sentiment_udf(F.col(\"text\")))\n",
    "\n",
    "    # å±•å¼€ç»“æ„ä½“å¹¶ä¿å­˜\n",
    "    final_sentiment = result_df.select(\n",
    "        \"news_id\",\n",
    "        F.col(\"sentiment.label\").alias(\"sentiment_label\"),\n",
    "        F.col(\"sentiment.score\").alias(\"sentiment_score\"),\n",
    "        F.col(\"sentiment.score_signed\").alias(\"sentiment_score_signed\")\n",
    "    )\n",
    "\n",
    "    final_sentiment.write.mode(\"overwrite\").parquet(SENTIMENT_OUTPUT)\n",
    "    print(\"âœ… Sentiment Analysis Complete:\", SENTIMENT_OUTPUT)\n",
    "    return final_sentiment\n",
    "\n",
    "def build_fact_news_sentiment(spark):\n",
    "    \"\"\"\n",
    "    ã€è¡¥å…¨ã€‘æ„å»ºæœ€ç»ˆäº‹å®è¡¨ (Fact Table)\n",
    "    å°† Silver å±‚çš„æ˜ç»†æ•°æ® ä¸ NLP æƒ…æ„Ÿåˆ†æ•° Join åœ¨ä¸€èµ·ã€‚\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ Building FACT_NEWS_SENTIMENT Table...\")\n",
    "    if os.path.exists(FACT_NEWS_SENTIMENT): shutil.rmtree(FACT_NEWS_SENTIMENT)\n",
    "\n",
    "    # 1. è¯»å– Silver (ä¸šåŠ¡æ•°æ®)\n",
    "    silver_df = spark.read.parquet(SILVER)\n",
    "\n",
    "    # 2. è¯»å– Sentiment (æƒ…æ„Ÿæ•°æ®)\n",
    "    sentiment_df = spark.read.parquet(SENTIMENT_OUTPUT)\n",
    "\n",
    "    # 3. Join æ“ä½œ\n",
    "    fact_df = silver_df.join(sentiment_df, on=\"news_id\", how=\"left\")\n",
    "\n",
    "    # 4. é€‰æ‹©æœ€ç»ˆéœ€è¦çš„åˆ— (é¢å‘åˆ†æ)\n",
    "    fact_df = fact_df.select(\n",
    "        \"news_id\", \"Date\", \"Stock_symbol\", \"Article_title\", \"Url\",\n",
    "        \"Publisher_norm\", \"Author_norm\",\n",
    "        \"sentiment_label\", \"sentiment_score\", \"sentiment_score_signed\"\n",
    "    )\n",
    "\n",
    "    fact_df.write.mode(\"overwrite\").parquet(FACT_NEWS_SENTIMENT)\n",
    "    print(\"âœ… FACT Table Created:\", FACT_NEWS_SENTIMENT)\n",
    "    return fact_df\n",
    "\n",
    "def build_gold_daily_agg(spark):\n",
    "    \"\"\"\n",
    "    Gold Layer: æ¯æ—¥èšåˆç»Ÿè®¡\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ Building Gold Aggregation (Daily Stats)...\")\n",
    "    if os.path.exists(GOLD): shutil.rmtree(GOLD)\n",
    "\n",
    "    df = spark.read.parquet(FACT_NEWS_SENTIMENT) # ä» Fact è¡¨è¯»å–æ›´æ–¹ä¾¿\n",
    "\n",
    "    gold_df = df.groupBy(\"Date\", \"Stock_symbol\").agg(\n",
    "        F.count(\"news_id\").alias(\"news_count\"),\n",
    "        F.avg(\"sentiment_score_signed\").alias(\"avg_sentiment\"),\n",
    "        F.countDistinct(\"Publisher_norm\").alias(\"publisher_count\")\n",
    "    )\n",
    "\n",
    "    gold_df.write.mode(\"overwrite\").parquet(GOLD)\n",
    "    print(\"âœ… Gold Aggregation Saved:\", GOLD)\n",
    "    return gold_df"
   ],
   "id": "511fd954d44b737f",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:48:32.260349Z",
     "start_time": "2025-11-28T03:48:32.254350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_to_snowflake(df, table_name):\n",
    "    \"\"\"\n",
    "    å°† Spark DataFrame å†™å…¥ Snowflake\n",
    "    éœ€ç¡®ä¿å·²æ­£ç¡®é…ç½® spark.jars.packages\n",
    "    \"\"\"\n",
    "    print(f\"â„ï¸ Saving to Snowflake table: {table_name}...\")\n",
    "\n",
    "    # è¯·æ›¿æ¢ä¸ºæ‚¨çš„å®é™…é…ç½®\n",
    "    sfOptions = {\n",
    "        \"sfURL\": \"https://<your_account>.snowflakecomputing.com\",\n",
    "        \"sfUser\": \"<your_username>\",\n",
    "        \"sfPassword\": \"<your_password>\",\n",
    "        \"sfDatabase\": \"YOUR_DB_NAME\",\n",
    "        \"sfSchema\": \"PUBLIC\",\n",
    "        \"sfWarehouse\": \"COMPUTE_WH\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        df.write \\\n",
    "            .format(\"net.snowflake.spark.snowflake\") \\\n",
    "            .options(**sfOptions) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "        print(f\"âœ… Successfully wrote to {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Snowflake write failed (Check JAR/Configs): {e}\")"
   ],
   "id": "9d541acc7185b3d7",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Main Execution Block\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. å¯åŠ¨ Spark\n",
    "    spark = create_spark()\n",
    "\n",
    "    # 2. åŸºç¡€ ETL\n",
    "    build_bronze(spark)\n",
    "    build_silver(spark)\n",
    "\n",
    "    # 3. NLP å¤„ç† (ä¼˜åŒ–ç‰ˆ)\n",
    "    build_silver_nlp(spark)\n",
    "    build_sentiment_features_scalable(spark)  # ä½¿ç”¨ Pandas UDF å¹¶è¡Œå¤„ç†\n",
    "\n",
    "    # 4. æ„å»ºäº‹å®è¡¨ (Join)\n",
    "    fact_df = build_fact_news_sentiment(spark)\n",
    "\n",
    "    # 5. æ„å»ºèšåˆè¡¨ (Gold)\n",
    "    gold_df = build_gold_daily_agg(spark)\n",
    "\n",
    "    # 6. ä¿å­˜åˆ° Snowflake (å¯é€‰ï¼Œéœ€å¡«å†™é…ç½®)\n",
    "    # save_to_snowflake(fact_df, \"FACT_NEWS_SENTIMENT\")\n",
    "    # save_to_snowflake(gold_df, \"GOLD_DAILY_STATS\")\n",
    "\n",
    "    print(\"ğŸ‰ All ELT Steps Completed Successfully!\")"
   ],
   "id": "1b03249708eda580"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:54:36.748119Z",
     "start_time": "2025-11-28T03:54:36.740533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def create_spark(app_name=\"5400-news-elt\"):\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(app_name)\n",
    "        .master(\"local[*]\")            # local CPU\n",
    "        .config(\"spark.driver.memory\", \"12g\")\n",
    "        .config(\"spark.executor.memory\", \"12g\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"400\")\n",
    "        .config(\"spark.default.parallelism\", \"400\")\n",
    "        .config(\"spark.sql.files.maxRecordsPerFile\", \"200000\")\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "        .config(\"spark.hadoop.fs.file.impl\", \"org.apache.hadoop.fs.RawLocalFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.file.impl.disable.cache\", \"true\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    print(\"ğŸ”¥ Spark Ready:\", spark.version)\n",
    "    return spark"
   ],
   "id": "23a4d9c4cef37392",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:54:38.671857Z",
     "start_time": "2025-11-28T03:54:38.652631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "BASE = r\"D:\\Columbia\\Fall2025\\5400\\project\\layer\\1\"\n",
    "CSV_PATH = r\"D:\\Columbia\\Fall2025\\5400\\project\\All_external.csv\"\n",
    "\n",
    "BRONZE = os.path.join(BASE, \"bronze\")\n",
    "SILVER = os.path.join(BASE, \"silver\")\n",
    "GOLD = os.path.join(BASE, \"gold\")\n",
    "NLP_SILVER = os.path.join(BASE, \"silver_for_nlp\")\n"
   ],
   "id": "cb4d11330cce8882",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:54:39.001220Z",
     "start_time": "2025-11-28T03:54:38.986668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_bronze(spark):\n",
    "    os.makedirs(BRONZE, exist_ok=True)\n",
    "\n",
    "    print(\"ğŸ“¥ Loading CSV...\")\n",
    "    df = (\n",
    "        spark.read.format(\"csv\")\n",
    "        .options(\n",
    "            header=\"true\",\n",
    "            inferSchema=\"true\",\n",
    "            multiLine=\"true\",\n",
    "            escape=\"\\\"\",\n",
    "            quote=\"\\\"\",\n",
    "            mode=\"PERMISSIVE\"\n",
    "        )\n",
    "        .load(CSV_PATH)\n",
    "    )\n",
    "\n",
    "    # Cleaning string columns\n",
    "    for c, t in df.dtypes:\n",
    "        if t == \"string\":\n",
    "            df = df.withColumn(c, F.trim(F.col(c)))\n",
    "\n",
    "    # date\n",
    "    if \"Date\" in df.columns:\n",
    "        df = df.withColumn(\"Date\", F.to_date(\"Date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "    df = df.repartition(400)\n",
    "\n",
    "    print(\"ğŸ’¾ Writing Bronze...\")\n",
    "    df.write.mode(\"overwrite\").parquet(BRONZE)\n",
    "    print(\"ğŸ¥‰ Bronze Ready:\", BRONZE)\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_silver(spark):\n",
    "    os.makedirs(SILVER, exist_ok=True)\n",
    "    df = spark.read.parquet(BRONZE)\n",
    "\n",
    "    # key fields not null\n",
    "    if \"Date\" in df.columns:\n",
    "        df = df.filter(F.col(\"Date\").isNotNull())\n",
    "    if \"Stock_symbol\" in df.columns:\n",
    "        df = df.filter(F.col(\"Stock_symbol\").isNotNull())\n",
    "\n",
    "    # drop duplicates\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    # standardize text fields\n",
    "    if \"Publisher\" in df.columns:\n",
    "        df = df.withColumn(\"Publisher_norm\", F.upper(\"Publisher\"))\n",
    "    if \"Author\" in df.columns:\n",
    "        df = df.withColumn(\"Author_norm\", F.upper(\"Author\"))\n",
    "\n",
    "    # unique ID\n",
    "    df = df.withColumn(\"news_id\", F.monotonically_increasing_id())\n",
    "\n",
    "    # repartition\n",
    "    df = df.repartition(400, \"Stock_symbol\", \"Date\")\n",
    "\n",
    "    print(\"ğŸ’¾ Writing Silver...\")\n",
    "    df.write.mode(\"overwrite\").parquet(SILVER)\n",
    "    print(\"ğŸ¥ˆ Silver Ready:\", SILVER)\n",
    "\n",
    "    return df\n",
    "\n",
    "def build_gold(spark):\n",
    "    os.makedirs(GOLD, exist_ok=True)\n",
    "    df = spark.read.parquet(SILVER)\n",
    "\n",
    "    df = df.withColumn(\"title_len\", F.length(\"Article_title\"))\n",
    "\n",
    "    agg_cols = [\n",
    "        F.count(\"*\").alias(\"article_count\"),\n",
    "        F.countDistinct(\"Publisher\").alias(\"publisher_count\"),\n",
    "        F.avg(\"title_len\").alias(\"avg_title_len\"),\n",
    "    ]\n",
    "\n",
    "    for col in [\"Textrank_summary\", \"Lsa_summary\", \"Luhn_summary\", \"Lexrank_summary\"]:\n",
    "        if col in df.columns:\n",
    "            agg_cols.append(F.first(col, ignorenulls=True).alias(\"sample_summary\"))\n",
    "            break\n",
    "\n",
    "    gold_df = df.groupBy(\"Date\", \"Stock_symbol\").agg(*agg_cols)\n",
    "    gold_df = gold_df.repartition(200, \"Stock_symbol\")\n",
    "\n",
    "    print(\"ğŸ’¾ Writing Gold...\")\n",
    "    gold_df.write.mode(\"overwrite\").parquet(GOLD)\n",
    "    print(\"ğŸ¥‡ Gold Ready:\", GOLD)\n",
    "\n",
    "    return gold_df"
   ],
   "id": "feccda5664b10e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T03:57:33.124767Z",
     "start_time": "2025-11-28T03:54:39.537320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = create_spark()\n",
    "\n",
    "bronze_df = build_bronze(spark)\n",
    "#silver_df = build_silver(spark)\n",
    "#gold_df   = build_gold(spark)\n",
    "#nlp_df    = build_silver_nlp(spark)"
   ],
   "id": "12f3464e815c63a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Spark Ready: 3.3.1\n",
      "ğŸ“¥ Loading CSV...\n",
      "ğŸ’¾ Writing Bronze...\n",
      "ğŸ¥‰ Bronze Ready: D:\\Columbia\\Fall2025\\5400\\project\\layer\\1\\bronze\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T04:04:33.876109Z",
     "start_time": "2025-11-28T04:04:33.858525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"test\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ],
   "id": "df933f9da33b14ef",
   "outputs": [],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
